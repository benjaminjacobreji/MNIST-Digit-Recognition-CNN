{"cells":[{"cell_type":"markdown","source":"# MNIST Character Recognition Demo\n A basic CNN to train on the MNIST dataset.","metadata":{"tags":[],"cell_id":"00000-d615690e-2e5d-4fa2-9ab5-93c3e6169ae2","output_cleared":false}},{"cell_type":"markdown","source":"Imports the TensorFlow library as it provides a high level API for creating CNN without adding the increaded complexity associated with Neural Networks.  \nTensorflow allow us to import and download the MNIST dataset directly from their API.","metadata":{"tags":[],"cell_id":"00001-85658af2-5509-4573-9031-8c7bfd23fa5e","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00000-58690166-5597-458c-8d6a-4653755eec11","output_cleared":false,"source_hash":"cc3eacf3","execution_millis":1436,"execution_start":1605420407174},"source":"import tensorflow as tf\n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()","execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"To visualize these numbers, we can get help from matplotlib.  \nWe basically have 60,000 images from the training dataset, to visualize. We use a random number generator to get an index to show.","metadata":{"tags":[],"cell_id":"00003-5cd8ff48-a804-4a75-a546-90d5702b90a5","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00001-cc5d61b3-4c7a-457d-ad46-3a99158d2dc9","output_cleared":false,"source_hash":"b1ae54a5","execution_millis":360,"execution_start":1605420408614},"source":"import matplotlib.pyplot as plt\nimport random\n# matplotlib inline # Only use this if using iPython\nimage_index = random.randint(0,x_train.shape[0]) # You may select anything up to 60,000\nprint(y_train[image_index]) # The label is 8\nplt.imshow(x_train[image_index], cmap='Greys')","execution_count":2,"outputs":[{"name":"stdout","text":"3\n","output_type":"stream"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f8824a33a10>"},"metadata":{}},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"markdown","source":"We can now see the shape of our dataset.  \nHere 60,000 and 10,000 represents the number of images in the train dataset.  \nWhile (28, 28) represents the size of the image: 28 x 28 pixel. ","metadata":{"tags":[],"cell_id":"00005-3cb8a630-33dd-45d2-8bb9-aaedc2231675","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-a752a4e6-1cfc-4ea9-931a-b0ae95497ad5","output_cleared":false,"source_hash":"2e85be79","execution_millis":3,"execution_start":1605420408977},"source":"print('x_train shape:', x_train.shape)\nprint('x_train shape:', x_test.shape)","execution_count":3,"outputs":[{"name":"stdout","text":"x_train shape: (60000, 28, 28)\nx_train shape: (10000, 28, 28)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here we first need to normalize the data before we can use it in a CNN.\nNormalizing is a process that changes the range of pixel intensity values.\nTo achieve the we divide it by the maximum RGB code 255.","metadata":{"tags":[],"cell_id":"00007-9e2f3e81-a1b1-44cd-a0c3-e1660a185ea5","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-f8439398-4c91-438a-9f49-777bad0ef2d5","output_cleared":false,"source_hash":"56503414","execution_millis":84,"execution_start":1605420408982},"source":"# Reshaping the array to 4-dims so that it can work with the Keras API\nx_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\nx_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\ninput_shape = (28, 28, 1)\n# Making sure that the values are float so that we can get decimal points after division\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n# Normalizing the RGB codes by dividing it to the max RGB value.\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint('Number of images in x_train', x_train.shape[0])\nprint('Number of images in x_test', x_test.shape[0])","execution_count":4,"outputs":[{"name":"stdout","text":"x_train shape: (60000, 28, 28, 1)\nNumber of images in x_train 60000\nNumber of images in x_test 10000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Main Part - The CNN itself\nWe use the Sequential Model from Keras and add Conv2D, MaxPooling, Flatten, Dropout, and Dense layers.  \nWe have already talked about Conv2D, Maxpooling, and Dense layers.  \nIn addition, Dropout layers fight with the overfitting by disregarding some of the neurons while training while Flatten layers flatten 2D arrays to 1D arrays before building the fully connected layers.  \nWe can experiment with the first Dense layer, BUT the final Dense layer must have 10 neurons since we have 10 number classes (0,1,2,...,9).","metadata":{"tags":[],"cell_id":"00009-7e481a15-dcdb-43fb-a28f-74940c686125","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-86618f25-4a5c-493c-9fed-b0c1efd568a9","output_cleared":false,"source_hash":"92063ffb","execution_millis":86,"execution_start":1605420409070},"source":"# Importing the required Keras modules containing model and layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n# Creating a Sequential Model and adding the layers\nmodel = Sequential()\nmodel.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten()) # Flattening the 2D arrays for fully connected layers\nmodel.add(Dense(128, activation=tf.nn.relu))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10,activation=tf.nn.softmax))","execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Up untl now we only have a non-optimized empty CNN.  \nNow we need to set an optimizer with a given loss funtion that uses a metric.  \nWe can experiment with the optimizer, loss function, metrics, and epochs.  \nHowever, the adam optimzer outperforms the other optimizers.  ","metadata":{"tags":[],"cell_id":"00011-1f1e1488-ea57-4a66-86b1-c4134f3b12db","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00005-910421aa-8fbe-4ea2-b0dd-015c2cb91556","output_cleared":false,"source_hash":"c033e220","execution_millis":4,"execution_start":1605420409163},"source":"model.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Print out to see what the model looks like","metadata":{"tags":[],"cell_id":"00013-ca9d032f-a864-4812-adf7-fcd6e2d3897f","output_cleared":false}},{"cell_type":"code","source":"print(model.summary())","metadata":{"tags":[],"cell_id":"00013-114ed154-ad76-47e8-b226-9c3407b7a121","output_cleared":false,"source_hash":"6f484792","execution_start":1605420409214,"execution_millis":1},"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 26, 26, 28)        280       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 13, 13, 28)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 4732)              0         \n_________________________________________________________________\ndense (Dense)                (None, 128)               605824    \n_________________________________________________________________\ndropout (Dropout)            (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 607,394\nTrainable params: 607,394\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Now we train or fit the model.  \nEpoch values, simply put is the number of iterations the data is passed through the CNN. As this is a relatively small dataset with an average of 30 seconds to complete one epoch, we can easily experiment with it to find the best values.","metadata":{"tags":[],"cell_id":"00014-640b6827-7067-464e-8c8e-aba6df8cc0d5","output_cleared":false}},{"cell_type":"code","source":"model.fit(x=x_train,y=y_train, epochs=10)","metadata":{"tags":[],"cell_id":"00014-c7cab897-51d4-490d-b0a0-04323b2f6da2","output_cleared":false,"source_hash":"441933a4","execution_start":1605420409215,"execution_millis":286887},"outputs":[{"name":"stdout","text":"Epoch 1/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.2143 - accuracy: 0.9351\nEpoch 2/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.0884 - accuracy: 0.9726\nEpoch 3/10\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.0597 - accuracy: 0.9813\nEpoch 4/10\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.0467 - accuracy: 0.9851\nEpoch 5/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.0368 - accuracy: 0.9881\nEpoch 6/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.0318 - accuracy: 0.9894\nEpoch 7/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.0273 - accuracy: 0.9908\nEpoch 8/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.0228 - accuracy: 0.9919\nEpoch 9/10\n1875/1875 [==============================] - 28s 15ms/step - loss: 0.0203 - accuracy: 0.9933\nEpoch 10/10\n1875/1875 [==============================] - 29s 15ms/step - loss: 0.0194 - accuracy: 0.9934\n","output_type":"stream"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f8826803a10>"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"Finally, we evaluate the trained model with x_test and y_test datasets.  \nWe see that for a basic model we have a pretty good accuracy of 98%.  ","metadata":{"tags":[],"cell_id":"00013-2855dfd8-c2bd-40c9-b954-678db60df23c","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00006-15ef94ea-95fb-4347-9618-87a5ca5442b3","output_cleared":false,"source_hash":"8434e76d","execution_millis":1795,"execution_start":1605420696103},"source":"model.evaluate(x_test, y_test)","execution_count":9,"outputs":[{"name":"stdout","text":"313/313 [==============================] - 2s 5ms/step - loss: 0.0619 - accuracy: 0.9831\n","output_type":"stream"},{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"[0.061851318925619125, 0.9830999970436096]"},"metadata":{}}]},{"cell_type":"markdown","source":"Now we can test it out and visualize if it recognizes characters or digits from the dataset.  \nEven if its hard for us to sometimes recognize the digit, the CNN is able to **predict what it most likely is**.","metadata":{"tags":[],"cell_id":"00015-53fdc701-cb27-405b-b32d-4cfe19c8cda8","output_cleared":false}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-99884614-c84a-4dda-85a4-0b67ade30643","output_cleared":false,"source_hash":"516adfba","execution_millis":124,"execution_start":1605420733134},"source":"image_index = random.randint(0,x_test.shape[0])\nplt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\npred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\nprint(pred.argmax())","execution_count":17,"outputs":[{"name":"stdout","text":"5\n","output_type":"stream"},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"637ab38f-ecb3-43b7-9396-bed58a8b97f1","deepnote_execution_queue":[]}}